tdm = as.matrix(tdm)
mode(tdm) = "numeric"
# transform into a term-term adjacency matrix
termMatrix <- tdm %*% t(tdm)
# inspect terms numbered 5 to 10
termMatrix[5:10,5:10]
##build a graph
library(igraph)
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
#simplify edges
g_backbone = getBackboneNetwork(g, alpha=0.00001, max.vertices=100)
vcount(g_backbone)
ecount(g_backbone)
##################
###Build a graph##
##################
# set seed to make the layout reproducible
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g_backbone)
# community detection
g.community<-walktrap.community(g_backbone)
# This algorithm uses random walks to find the most densely connected subgraphs.
members<-membership(g.community)
plot.igraph(g_backbone, layout=layout1)
##plot attribute
#vertex size -> word frequency
#collapse matrix by summing over columns
freq <- rowSums(as.matrix(tdm))
freq = data.frame(freq)
V(g_backbone)$label.cex <- (V(g_backbone)$degree+0.1) / (max(V(g_backbone)$degree)+1)
par(mar=c(.1,.1,.1,.1))    # sets the edges of the plotting area
plot.igraph(g_backbone,
layout=layout1,
vertex.size = 0,
vertex.label.cex=V(g_backbone)$label.cex*1.3,
vertex.frame.color = NA,
vertex.color = NA,
vertex.label.color = ifelse(members==1,"red","blue"),
edge.arrow.size=.5
)
library('visNetwork')
g_backbone_vis <- toVisNetworkData(g_backbone)
nodes = g_backbone_vis$nodes
tech  =  visNetwork(nodes, edges = g_backbone_vis$edges)
tech %>% visSave(file = "wireline_tvdetractor.html")
tech %>% visSave(file = "wireline_phonedetractor.html")
tech
TV_detractor = subset(trust,Segment == "TV" & NPS_Recode == "Detractor")
trustCorpus <- Corpus(VectorSource(TV_detractor$NPS2))
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
trustCorpus <- tm_map(trustCorpus, toSpace, "-")
trustCorpus <- tm_map(trustCorpus, toSpace, "'")
trustCorpus <- tm_map(trustCorpus, toSpace, "'")
trustCorpus <- tm_map(trustCorpus, toSpace, "^")
trustCorpus <- tm_map(trustCorpus, removeNumbers)
trustCorpus <- tm_map(trustCorpus, removePunctuation)
trustCorpus <- tm_map(trustCorpus, removeWords, c(stopwords('english'),"anything","can","one","want","may","will","think","use","way","anyth","alway",
"too","don","don't","know","seem","there's","there","thing","make","happen","take","much","none","also",
"already","the","there","someth","someo","feel","cause","become","lot","just","now","sure","people",
"need","like","mani","get","work","everthing","everth","becaus"))
trustCorpus <- tm_map(trustCorpus, stemDocument)
trustCorpus <- tm_map(trustCorpus, PlainTextDocument)
##check
writeLines(as.character(trustCorpus[[3]]))
###for text network analysis
#first create a TDM (term document matrix)
dtm= DocumentTermMatrix(trustCorpus,control=list(bounds = list(global = c(50,Inf))))
removeCommonTerms(dtm ,.8)
#convert rownames to filenames
rownames(dtm) <- filenames
#collapse matrix by summing over columns
freq <- colSums(as.matrix(dtm))
#length should be total number of terms
length(freq)
#create sort order (descending)
ord <- order(freq,decreasing=TRUE)
#List all terms in decreasing order of freq and write to disk
freq[ord]
write.csv(freq[ord],"word_freq_nps.csv")
###delete docs without words after processing
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ]           #remove all docs without words
#######################
###Layout for igraph###
#######################
##create a termdocMatrix for adjacency matrix
### after Terms that appear in <10 documents are discarded
tdm= TermDocumentMatrix(trustCorpus,control=list(bounds = list(global = c(50,Inf))))
tdm[5:10,1:20]
tdm = as.matrix(tdm)
mode(tdm) = "numeric"
# transform into a term-term adjacency matrix
termMatrix <- tdm %*% t(tdm)
# inspect terms numbered 5 to 10
termMatrix[5:10,5:10]
##build a graph
library(igraph)
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
#simplify edges
g_backbone = getBackboneNetwork(g, alpha=0.00001, max.vertices=100)
vcount(g_backbone)
ecount(g_backbone)
##################
###Build a graph##
##################
# set seed to make the layout reproducible
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g_backbone)
# community detection
g.community<-walktrap.community(g_backbone)
# This algorithm uses random walks to find the most densely connected subgraphs.
members<-membership(g.community)
plot.igraph(g_backbone, layout=layout1)
##plot attribute
#vertex size -> word frequency
#collapse matrix by summing over columns
freq <- rowSums(as.matrix(tdm))
freq = data.frame(freq)
V(g_backbone)$label.cex <- (V(g_backbone)$degree+0.1) / (max(V(g_backbone)$degree)+1)
par(mar=c(.1,.1,.1,.1))    # sets the edges of the plotting area
plot.igraph(g_backbone,
layout=layout1,
vertex.size = 0,
vertex.label.cex=V(g_backbone)$label.cex*1.3,
vertex.frame.color = NA,
vertex.color = NA,
vertex.label.color = ifelse(members==1,"red","blue"),
edge.arrow.size=.5
)
library('visNetwork')
g_backbone_vis <- toVisNetworkData(g_backbone)
nodes = g_backbone_vis$nodes
tech  =  visNetwork(nodes, edges = g_backbone_vis$edges)
tech %>% visSave(file = "wireline_tvdetractor.html")
install.packages("flexdashboard")
library(flexdashboard)
tech  =  visNetwork(nodes, edges = g_backbone_vis$edges)
install.packages("shiny")
library(shiny)
runExample("01_hello")
runApp("dashboard")
runApp("dashboard")
trust <- read.csv("C:/Users/xiyan_wang/Text/Wireline Data for Concept Mining.csv", dec=",", comment.char="#")
trust = subset(trust, NPS_Recode == sprintf("/%s", nps_cat))
runApp("dashboard")
install.packages('rsconnect')
library(rsconnect)
rsconnect::setAccountInfo(name='jamiewan',
token='66F2124DABCCCABE56A76DC5D5357E59',
secret='<SECRET>')
rsconnect::setAccountInfo(name='jamiewan',
token='66F2124DABCCCABE56A76DC5D5357E59',
secret='<SECRET>')
rsconnect::setAccountInfo(name='jamiewan',
token='66F2124DABCCCABE56A76DC5D5357E59',
secret='kAZAVk67hyV/QYVLxOzGDkxK7IvG01J54K3ys5Fk')
library(rsconnect)
rsconnect::deployApp('C:/Users/xiyan_wang/Text/dashboard')
library(rsconnect)
rsconnect::deployApp('C:/Users/xiyan_wang/Text/dashboard')
runApp(dashboard)
runApp("dashboard")
runApp("dashboard")
shiny::runApp(system.file("shiny", package = "visNetwork"))
rownames(dtm) <- filenames
removeCommonTerms(dtm ,.8)
rownames(dtm) <- filenames
trust <- read.csv("C:/Users/xiyan_wang/Text/Wireline Data for Concept Mining.csv", dec=",", comment.char="#")
Phone_detractor = subset(trust,Segment == "Phone" & NPS_Recode == "Detractor")
trustCorpus <- Corpus(VectorSource(TV_detractor$NPS2))
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
trustCorpus <- tm_map(trustCorpus, toSpace, "-")
trustCorpus <- tm_map(trustCorpus, toSpace, "'")
trustCorpus <- tm_map(trustCorpus, toSpace, "'")
trustCorpus <- tm_map(trustCorpus, toSpace, "^")
trustCorpus <- tm_map(trustCorpus, removeNumbers)
trustCorpus <- tm_map(trustCorpus, removePunctuation)
trustCorpus <- tm_map(trustCorpus, removeWords, c(stopwords('english'),"anything","can","one","want","may","will","think","use","way","anyth","alway",
"too","don","don't","know","seem","there's","there","thing","make","happen","take","much","none","also",
"already","the","there","someth","someo","feel","cause","become","lot","just","now","sure","people",
"need","like","mani","get","work","everthing","everth","becaus"))
trustCorpus <- tm_map(trustCorpus, stemDocument)
trustCorpus <- tm_map(trustCorpus, PlainTextDocument)
##check
writeLines(as.character(trustCorpus[[3]]))
###for text network analysis
#first create a TDM (term document matrix)
dtm= DocumentTermMatrix(trustCorpus,control=list(bounds = list(global = c(50,Inf))))
removeCommonTerms(dtm ,.8)
removeCommonTerms(dtm ,.8)
rownames(dtm) <- filenames
freq <- colSums(as.matrix(dtm))
require(tm)
require(SnowballC)
library(ggplot2)
library(lsa)
library(scatterplot3d)
library(devtools)
library(wordcloud)
trustCorpus <- Corpus(VectorSource(TV_detractor$NPS2))
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
trustCorpus <- tm_map(trustCorpus, toSpace, "-")
trustCorpus <- tm_map(trustCorpus, toSpace, "'")
trustCorpus <- tm_map(trustCorpus, toSpace, "'")
trustCorpus <- tm_map(trustCorpus, toSpace, "^")
trustCorpus <- tm_map(trustCorpus, removeNumbers)
trustCorpus <- tm_map(trustCorpus, removePunctuation)
trustCorpus <- tm_map(trustCorpus, removeWords, c(stopwords('english'),"anything","can","one","want","may","will","think","use","way","anyth","alway",
"too","don","don't","know","seem","there's","there","thing","make","happen","take","much","none","also",
"already","the","there","someth","someo","feel","cause","become","lot","just","now","sure","people",
"need","like","mani","get","work","everthing","everth","becaus"))
trustCorpus <- tm_map(trustCorpus, stemDocument)
trustCorpus <- tm_map(trustCorpus, PlainTextDocument)
##check
writeLines(as.character(trustCorpus[[3]]))
###for text network analysis
#first create a TDM (term document matrix)
dtm= DocumentTermMatrix(trustCorpus,control=list(bounds = list(global = c(50,Inf))))
removeCommonTerms(dtm ,.8)
#convert rownames to filenames
rownames(dtm) <- filenames
freq <- colSums(as.matrix(dtm))
#length should be total number of terms
length(freq)
#create sort order (descending)
ord <- order(freq,decreasing=TRUE)
#List all terms in decreasing order of freq and write to disk
freq[ord]
write.csv(freq[ord],"word_freq_nps.csv")
###delete docs without words after processing
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ]           #remove all docs without words
tdm= TermDocumentMatrix(trustCorpus,control=list(bounds = list(global = c(50,Inf))))
tdm[5:10,1:20]
tdm = as.matrix(tdm)
mode(tdm) = "numeric"
# transform into a term-term adjacency matrix
termMatrix <- tdm %*% t(tdm)
# inspect terms numbered 5 to 10
termMatrix[5:10,5:10]
##build a graph
library(igraph)
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
#simplify edges
g_backbone = getBackboneNetwork(g, alpha=0.00001, max.vertices=100)
vcount(g_backbone)
ecount(g_backbone)
################
###Build a graph##
##################
# set seed to make the layout reproducible
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g_backbone)
library('visNetwork')
g_backbone_vis <- toVisNetworkData(g_backbone)
nodes = g_backbone_vis$nodes
tech  =  visNetwork(nodes, edges = g_backbone_vis$edges)
tech %>% visSave(file = "wireline_tvdetractor.html")
tech
shiny::runApp(system.file("shiny", package = "visNetwork"))
save(g_backbone_vis, file="promoter.rda")
g_backbone =load("promoter.rda")
runApp('dashboard')
g_backbone = getBackboneNetwork(g, alpha=0.00001, max.vertices=100)
g_backbone_vis <- toVisNetworkData(g_backbone)
nodes = g_backbone_vis$nodes
View(nodes)
tech  =  visNetwork(nodes, edges = g_backbone_vis$edges)
tech = load('wireline_detractor')
tech = load('wireline_detractor.html')
?renderUI
runApp("dashboard")
runApp("dashboard")
getPage<-function() {
return(includeHTML("wireline_detractor.html"))
}
getPage()
g = saveRDS(g_backbone)
runApp('dashboard')
typeof(g_backbone)
save(g_backbone,file = 'promoter')
load(g_backbone,file = 'promoter')
load(file = 'promoter')
promoter
load(g_backbone,file = 'promoter.rda')
save(g_backbone,file = 'promoter.rda')
load(file = "promoter")
s= load(file = "promoter")
s
type(s)
typeof(s)
saveRDS(g_backbone,file = 'promoter.rda')
s= loadRDS(file = "promoter")
s= readRDS(file = "promoter")
s= readRDS(file = "promoter.rda")
s
runApp('dashboard')
runApp('dashboard')
g_backbone = readRDS(file = "promoter.rda")
g_backbone = readRDS(file = "promoter.rda")
g_backbone_vis <- toVisNetworkData(g_backbone)
nodes = g_backbone_vis$nodes
visNetwork(nodes, edges = g_backbone_vis$edges)
runApp('dashboard')
runApp('dashboard')
g_backbone = readRDS(file = (sprintf("./%s.rda", nps_cat)))
nps_cat <<-list("promoter","detractor")
g_backbone = readRDS(file = (sprintf("./%s.rda", nps_cat)))
nps_cat <<-list("promoter" = "promoter","detractor" = "detractor")
g_backbone = readRDS(file = (sprintf("./%s.rda", nps_cat)))
getIgraph <- memoise(function(nps_cat) {
# Careful not to let just any name slip in here; a
# malicious user could manipulate this value.
if (!(nps_cat %in% nps_cat))
stop("Unknown nps_cat")
igraph <- readRDS(file = sprintf("./%s.rda", nps_cat),
encoding="UTF-8")
})
runApp('dashboard')
shiny::runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
Phone_detractor = subset(trust,Segment == "Phone" & NPS_Recode == "Detractor")
trustCorpus <- Corpus(VectorSource(Phone_detractor$NPS2))
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
trustCorpus <- tm_map(trustCorpus, toSpace, "-")
trustCorpus <- tm_map(trustCorpus, toSpace, "'")
trustCorpus <- tm_map(trustCorpus, toSpace, "'")
trustCorpus <- tm_map(trustCorpus, toSpace, "^")
trustCorpus <- tm_map(trustCorpus, removeNumbers)
trustCorpus <- tm_map(trustCorpus, removePunctuation)
trustCorpus <- tm_map(trustCorpus, removeWords, c(stopwords('english'),"anything","can","one","want","may","will","think","use","way","anyth","alway",
"too","don","don't","know","seem","there's","there","thing","make","happen","take","much","none","also",
"already","the","there","someth","someo","feel","cause","become","lot","just","now","sure","people",
"need","like","mani","get","work","everthing","everth","becaus"))
trustCorpus <- tm_map(trustCorpus, stemDocument)
trustCorpus <- tm_map(trustCorpus, PlainTextDocument)
require(tm)
require(SnowballC)
library(ggplot2)
library(lsa)
library(devtools)
library(wordcloud)
install.packages("curl")
Phone_detractor = subset(trust,Segment == "Phone" & NPS_Recode == "Detractor")
trustCorpus <- Corpus(VectorSource(Phone_detractor$NPS2))
toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))})
trustCorpus <- tm_map(trustCorpus, toSpace, "-")
trustCorpus <- tm_map(trustCorpus, toSpace, "'")
trustCorpus <- tm_map(trustCorpus, toSpace, "'")
trustCorpus <- tm_map(trustCorpus, toSpace, "^")
trustCorpus <- tm_map(trustCorpus, removeNumbers)
trustCorpus <- tm_map(trustCorpus, removePunctuation)
trustCorpus <- tm_map(trustCorpus, removeWords, c(stopwords('english'),"anything","can","one","want","may","will","think","use","way","anyth","alway",
"too","don","don't","know","seem","there's","there","thing","make","happen","take","much","none","also",
"already","the","there","someth","someo","feel","cause","become","lot","just","now","sure","people",
"need","like","mani","get","work","everthing","everth","becaus"))
trustCorpus <- tm_map(trustCorpus, stemDocument)
trustCorpus <- tm_map(trustCorpus, PlainTextDocument)
##check
writeLines(as.character(trustCorpus[[3]]))
###for text network analysis
#first create a TDM (term document matrix)
dtm= DocumentTermMatrix(trustCorpus,control=list(bounds = list(global = c(50,Inf))))
removeCommonTerms(dtm ,.8)
#convert rownames to filenames
#rownames(dtm) <- filenames
#collapse matrix by summing over columns
freq <- colSums(as.matrix(dtm))
#length should be total number of terms
length(freq)
#create sort order (descending)
ord <- order(freq,decreasing=TRUE)
#List all terms in decreasing order of freq and write to disk
freq[ord]
write.csv(freq[ord],"word_freq_nps.csv")
###delete docs without words after processing
rowTotals <- apply(dtm , 1, sum) #Find the sum of words in each Document
dtm.new   <- dtm[rowTotals> 0, ]           #remove all docs without words
#######################
###Layout for igraph###
#######################
##create a termdocMatrix for adjacency matrix
### after Terms that appear in <10 documents are discarded
tdm= TermDocumentMatrix(trustCorpus,control=list(bounds = list(global = c(50,Inf))))
tdm[5:10,1:20]
tdm = as.matrix(tdm)
mode(tdm) = "numeric"
# transform into a term-term adjacency matrix
termMatrix <- tdm %*% t(tdm)
# inspect terms numbered 5 to 10
termMatrix[5:10,5:10]
##build a graph
library(igraph)
# build a graph from the above matrix
g <- graph.adjacency(termMatrix, weighted=T, mode = "undirected")
# remove loops
g <- simplify(g)
# set labels and degrees of vertices
V(g)$label <- V(g)$name
V(g)$degree <- degree(g)
#simplify edges
g_backbone = getBackboneNetwork(g, alpha=0.00001, max.vertices=100)
vcount(g_backbone)
ecount(g_backbone)
##################
###Build a graph##
##################
# set seed to make the layout reproducible
set.seed(3952)
layout1 <- layout.fruchterman.reingold(g_backbone)
install_github("kasperwelbers/semnet")
library(semnet)
g_backbone = getBackboneNetwork(g, alpha=0.00001, max.vertices=100)
saveRDS(g_backbone,file = "detractor.rda")
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
getIgraph('promoter')
runApp('dashboard')
getIgraph('promoter')
igraph <- readRDS(file = sprintf("%s.rda", nps_cat))
runApp('dashboard')
runApp('dashboard')
?sprintf
nps_cat = "promoter"
igraph <- readRDS(file = sprintf(".%s.rda", nps_cat))
igraph <- readRDS(file = sprintf("%s.rda", nps_cat))
runApp('dashboard')
rm(igraph)
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
library(memoise)
nps_cats <<-list("promoter" = "promoter","detractor" = "detractor")
getIgraph <- memoise(function(nps_cat) {
# Careful not to let just any name slip in here; a
# malicious user could manipulate this value.
if (!(nps_cat %in% nps_cats))
stop("Unknown nps_cat")
backbone <- readRDS(file = sprintf("%s.rda", nps_cat))
})
getIgraph(nps_cats[1])
backbone
getIgraph("promoter")
backbone
?print
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
plot.igraph(backbone)
plot.igraph(g_backbone)
runApp('dashboard')
tech %>% visSave(file = "wireline_tvdetractor.html")
tech
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
library(rsconnect)
deployApp()
deployApp()
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
runApp('dashboard')
install.packages("doParallel")
